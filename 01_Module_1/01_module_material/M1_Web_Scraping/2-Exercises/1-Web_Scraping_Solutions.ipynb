{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1-Web_Scraping_Solutions.ipynb","provenance":[{"file_id":"1J5sCNUxmBh0t7MH3cVHColt9WFjca3ze","timestamp":1614455032952}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U3zPuM3zuwFT"},"source":["### Web Scraping##\n","#Exercises#"]},{"cell_type":"markdown","metadata":{"id":"ihjiiXFWCoOL"},"source":["\n","\n","---\n","\n","\n","<font color='violet'>\n","Hints are written in white, so you do not see them immediately. If you highlight them (or double-click on them), they will appear! \n","<font color='white'> I am a hint! :-)\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"U2hSLoyOBc1C"},"source":["## 1. Basic exercises"]},{"cell_type":"markdown","metadata":{"id":"fTXm4BjVXSwH"},"source":["### Exercise 1.1"]},{"cell_type":"markdown","metadata":{"id":"4LigdyQsguzj"},"source":["Import the ``requests`` library, the ``BeautifulSoup`` library and the ``pandas`` library."]},{"cell_type":"code","metadata":{"id":"OP6Rw0trhTGl"},"source":["import pandas\n","import requests\n","from bs4 import BeautifulSoup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikjlp_1mhiQT"},"source":["Using the ``requests`` library, retrieve the example page (http://repec.sowi.unibe.ch/varia/example-page.html) and assign the response object to a variable named ``exR``. Print out the status code. You will get a  number. What does it mean?"]},{"cell_type":"code","metadata":{"id":"ipgty3mXi5JE"},"source":["exR = requests.get(\"http://repec.sowi.unibe.ch/varia/example-page.html\")\n","exR.status_code # Status code of 200 means that everything went well "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2upIicTWjvAi"},"source":["Now print out the text of the response."]},{"cell_type":"code","metadata":{"id":"AI3TosUojy1x"},"source":["exR.text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrppfJNNks2D"},"source":["### Exercise 1.2"]},{"cell_type":"markdown","metadata":{"id":"4EvpnIcBlxbK"},"source":["Using ``BeautifulSoup``, parse the text of your response object and assign the result to a variable called ``mySoup``."]},{"cell_type":"code","metadata":{"id":"lBCQksmamxtn"},"source":["mySoup = BeautifulSoup(exR.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAMXMHCBmdzu"},"source":["Print the content of your soup object."]},{"cell_type":"code","metadata":{"id":"wcDEhu4QmyfF"},"source":["print(mySoup.prettify())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Sn-baMZmzIF"},"source":["Now, try to access the following:\n","1. The ``thead`` element\n","2. All ``p`` elements\n","3. The last ``p`` element\n","4. The text of the ``h1`` element\n","5. The first URL in the document (only the URL!)\n","6. All ``a`` elements within the ``table`` element\n","7. All ``table`` elements of class \"cat_table\"  <font color='violet'> Hint: <font color='white'>  Remember to use class_ instead of class to find elements based on the value of the class attribute!<font color='black'> \n","8. The text of all ``p`` elements (as a list) <font color='violet'> Hint: <font color='white'>  Use a list comprehension! <font color='black'> \n","\n"]},{"cell_type":"code","metadata":{"id":"bc64FjCDo-HQ"},"source":["# 1: The thead element\n","mySoup.find(\"thead\") # Or just: mySoup.thead"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZM9YGDWo-B3"},"source":["# 2: All p elements\n","mySoup.find_all(\"p\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QiRdk_To96f"},"source":["# 3: The last p element\n","mySoup.find_all(\"p\")[-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBXHgh79o9zm"},"source":["# 4: The text of the h1 element\n","mySoup.find(\"h1\").get_text() # Or: mySoup.h1.text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZhL7gtfo9sv"},"source":["# 5: The first URL in the document (only the URL!) \n","mySoup.a[\"href\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkpR1GiFo9lo"},"source":["# 6: All a elements within the table element\n","mySoup.find(\"table\").find_all(\"a\") # Or just: mySoup.table(\"a\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmFdainGmmNn"},"source":["# 7: All table elements of class \"cat_table\" \n","mySoup.find_all(\"table\", class_=\"cat_table\") # Or:\n","mySoup.find_all(\"table\", attrs = {\"class\" : \"cat_table\"})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVWKcYleo9eH"},"source":["# 8: The text of all p elements (as a list)\n","aElems = mySoup.find_all(\"p\")\n","urls = [elem.get_text() for elem in aElems]\n","urls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYEvEr7QABjT"},"source":["### Exercise 1.3"]},{"cell_type":"markdown","metadata":{"id":"s3U_Q7V4AF9B"},"source":["Most websites are a bit more complicated than our example page. In this exercise, we will retrieve the Wikipedia page on cats: https://en.wikipedia.org/wiki/Cat"]},{"cell_type":"markdown","metadata":{"id":"cSRj3gIpAdgs"},"source":["Retrieve the page, get the text and convert it to a BeautifulSoup object called ``cats``."]},{"cell_type":"code","metadata":{"id":"MKgkt-GTA3Tw"},"source":["res = requests.get(\"https://en.wikipedia.org/wiki/Cat\")\n","cats = BeautifulSoup(res.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y00ikW9ABmSo"},"source":["Go to the page and inspect it (right-click on the different elements and select \"Insepct\" (Element untersuchen)\". Then, try to retrieve the following elements from the page:\n","\n","1. The title of the page (only the text)\n","2. The title header of the page (Cat)\n","3.  All the main headers of the text on the page (Etymology and naming, Taxonomy...)\n","4. All the headers in the text (Etymology and naming, Taxonomy, Evolution, Domestication, Characteristics, Size...)\n"," <font color='violet'> Hint: <font color='white'> These headers are all of the same class.<font color='black'>\n","5. The opening paragraph (\"The cat (Felis catus) is a ...\")\n","6. All the links in the infobox table on the right \n","7. The number of images on the page <font color='violet'> Hint: <font color='white'> Hint: You can use the len() function"]},{"cell_type":"code","metadata":{"id":"1BNAIflGCoQ_"},"source":["# 1\n","cats.find(\"title\").get_text() # Or: cats.title.text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RarP03dGDk3J"},"source":["# 2\n","cats.find(\"h1\").get_text() # Or: cats.h1.text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZn0L62BDuSw"},"source":["# 3\n","headers = [header.text for header in cats.find_all(\"h2\")]\n","headers "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_m88q-Z5GgnN"},"source":["# 4\n","[header.text for header in cats.find_all(class_=\"mw-headline\")] \n","# Or: [header.text for header in cats.find_all([\"h1\", \"h2\", \"h3\"])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckYb0J_8KoeP"},"source":["# 5\n","cats.find_all(\"p\")[1].text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEolGY--HYr8"},"source":["# 6\n","links = cats.find(\"table\", class_=\"infobox biota\").find_all(\"a\")\n","links = [link[\"href\"] for link in links] # Or:[\"https://en.wikipedia.org\" + link[\"href\"] for link in links]\n","links"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IofJLYJHbgSo"},"source":["# 7\n","len(cats.find_all(\"img\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH1bxuwGAfqA"},"source":["Now try to retrieve the first table on the page and convert it to a Pandas dataframe."]},{"cell_type":"code","metadata":{"id":"1U3PBtgQAgX2"},"source":["import pandas as pd\n","cat_table = pd.read_html(\"https://en.wikipedia.org/wiki/Cat\")[0]\n","cat_table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zesfc5Otne-8"},"source":["### Exercise 1.4"]},{"cell_type":"markdown","metadata":{"id":"tb69L2RcN0Hn"},"source":["Consider the following list of links to Wikipedia pages on animals:"]},{"cell_type":"code","metadata":{"id":"GavxRwQvN8Rt"},"source":["animals_wiki = [\"https://en.wikipedia.org/wiki/Cat\",\n","                \"https://en.wikipedia.org/wiki/Dog\",\n","                \"https://en.wikipedia.org/wiki/Tiger\",\n","                \"https://en.wikipedia.org/wiki/Panda\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wxV20wlP8ql"},"source":["Write a simple loop that fetches each of these pages and writes the response into a list (it should look like this: ``[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>]``)"]},{"cell_type":"code","metadata":{"id":"EeeWVHQ8Nz0X"},"source":["L = []\n","for link in animals_wiki:\n","  r = requests.get(link)\n","  L.append(r)\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRc8cXUEnkDU"},"source":["You would like to have (1) the title header and (2) the number of images of each of these pages. Revise your loop so that this information is retreived from the source code of each page and written into a (nested) list. Which animal page has the most images?"]},{"cell_type":"code","metadata":{"id":"pLIaoPb7njfE"},"source":["L = []\n","for link in animals_wiki:\n","  r = requests.get(link)\n","  soup = BeautifulSoup(r.text)\n","\n","  header = soup.h1.text\n","  nr_images = len(soup.find_all(\"img\"))\n","\n","  L.append([header, nr_images])\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ak6TzkmZUDj"},"source":["# Tiger page has the most images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITCk8WoQTfYW"},"source":["Repetition task: Convert your list into a pandas dataframe."]},{"cell_type":"code","metadata":{"id":"Ehbg7j1uTKNp"},"source":["animal_data = pd.DataFrame(L, columns=[\"Animal\", \"Nr. of images\"])\n","animal_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I5fGuENsxWXa"},"source":["## 2. Advanced exercises*"]},{"cell_type":"markdown","metadata":{"id":"S4EPeADLLUwr"},"source":["---\n","\n","\n","<font color='red'>\n","*Feel free to skip the advanced exercises if you feel overwhelmed or if trying to solve the basic exercises already took you a lot of time! \n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZjVQNxR4daUW"},"source":["### Exercise 2.1"]},{"cell_type":"markdown","metadata":{"id":"a-NbMd9KfEu1"},"source":["Suppose your list of animal links also contains a link to a website that does not exist: "]},{"cell_type":"code","metadata":{"id":"2xiYa5zZfB7e"},"source":["animals_wiki = [\"https://en.wikipedia.org/wiki/Cat\",\n","                \"https://en.wikipedia.org/wiki/Dog\",\n","                \"https://no-such-link-exists.com\",\n","                \"https://en.wikipedia.org/wiki/Panda\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vf7QMF29fJd8"},"source":["Add a ``try-exept`` block to the loop from Exercise 1.4 to prevent your web scraper from crashing when an URL cannot be retrieved. <font color='violet'> Hint: <font color='white'> Use ``continue`` within the ``except`` block to jump back to the beginning of the loop!\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"APvssG-KfLcS"},"source":["L = []\n","for link in animals_wiki:\n","  try:\n","    r = requests.get(link)\n","  except:\n","    print(\"Problem loading the site:\", link)\n","    continue # Go back to beginning of the for loop\n","\n","  soup = BeautifulSoup(r.text)\n","\n","  header = soup.h1.text\n","  nr_images = len(soup.find_all(\"img\"))\n","\n","  L.append([link, header, nr_images]) # append url as well so you know which \n","                                      # site corresponds to which url\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1es3ky6e9_q"},"source":["### Exercise 2.2"]},{"cell_type":"markdown","metadata":{"id":"QfMgxGeJcw--"},"source":["The Wikipedia page https://en.wikipedia.org/wiki/List_of_cat_breeds contains a list of all cat breeds and the links to the respective Wikipedia pages. You would like to create a dataset about the different cat breeds with information from their Wikipedia pages.\n","\n","In a first step, you will have to retrieve all the links to the respective Wikipedia pages. Retrieve them from the first table on the website and write them into a list. *Note that the table also contains some links you do not want to have included (you only want those to the pages for the different cat breeds). You can use ``CSS`` selectors to specify what links you want to extract. First have a look at the source code of the page to find out how the relevant links can be addressed.*"]},{"cell_type":"code","metadata":{"id":"oFgyXzHxVkSv"},"source":["# Request page and make soup object\n","cat_res = requests.get(\"https://en.wikipedia.org/wiki/List_of_cat_breeds\")\n","cat_soup = BeautifulSoup(cat_res.text)\n","\n","# Select the table\n","table = cat_soup.find_all(\"table\")[0]\n","\n","# Select a elements that are directly within a table head that is within a table row\n","cat_links = table.select(\"tr th > a\")\n","\n","# Extract URLs\n","cat_links = [link[\"href\"] for link in cat_links] \n","cat_links"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ch3yRDKkf6o3"},"source":["If you inspect your list, you may notice that there are some external URLs (i.e. URLs that do not point to Wikipedia pages). Try to remove them! <font color='violet'> Hint: <font color='white'> Note that you can specify an if-condition within a list comprehension. In the very first tutorial you learned how to check if a string is containend within anotther string."]},{"cell_type":"code","metadata":{"id":"AUihku13baXZ"},"source":["# Remove non-wiki links\n","cat_links = [link for link in cat_links if \"wiki\" in link] \n","cat_links"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66cRQYqNcvov"},"source":["With your cleaned list you can now start to scrape the pages. You would like to retrieve (1) the title header, (2) the number of images, (3) the number of characters of the text of each page, and (4) the text of the introductory paragraph. Try to do so for the **first page only** to develop your code.  Note that you will have to add the top level domain (https://en.wikipedia.org) to your URL!"]},{"cell_type":"code","metadata":{"id":"g6S-P27ojJIR"},"source":["link = \"https://en.wikipedia.org\" + cat_links[0]\n","\n","r = requests.get(link)\n","soup = BeautifulSoup(r.text)\n","\n","header = soup.h1.text\n","nr_images = len(soup.find_all(\"img\"))\n","chars = len(soup.body.text)\n","par1 = soup.find_all(\"p\")[1].text\n","\n","[header, nr_images, chars, par1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdIOFIzYjcs9"},"source":["Now you are ready to build your web scraper. Write a loop the fetches the information from all the pages and writes it into a list. Tipp: Before you loop through the entire list, try looping over the first few elements to check if everything works (running the loop across the whole list may take a while)."]},{"cell_type":"code","metadata":{"id":"S9Gm5IlpoWS3"},"source":["L = []\n","\n","for subdom in cat_links:\n","\n","  link = \"https://en.wikipedia.org\" + subdom\n","  r = requests.get(link)\n","\n","  soup = BeautifulSoup(r.text)\n","\n","  header = soup.h1.text\n","  nr_images = len(soup.find_all(\"img\"))\n","  chars = len(soup.body.text)\n","  par1 = soup.find_all(\"p\")[1].text\n","  \n","  L.append([header, nr_images, chars, par1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkhfVmTNlQZf"},"source":["# More robust and slower version (not necessary in this case)\n","import time \n","L = []\n","\n","for subdom in cat_links:\n","  time.sleep(1)\n","  link = \"https://en.wikipedia.org\" + subdom\n","\n","  try:\n","    r = requests.get(link)\n","  except Exception as e:\n","    print(\"Error with:\", link)\n","    print(e)\n","\n","  soup = BeautifulSoup(r.text)\n","\n","  header = soup.h1.text\n","  nr_images = len(soup.find_all(\"img\"))\n","  chars = len(soup.body.text)\n","  par1 = soup.find_all(\"p\")[1].text\n","\n","  L.append([header, nr_images, chars, par1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GmUHDnIJoJnQ"},"source":["# Look at some example results\n","L[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6S7NWWYjn6c"},"source":["Create a pandas Dataframe with the information you gathered and inspect it. Which cat has the longest article? Which one has the most images?"]},{"cell_type":"code","metadata":{"id":"ebz0FVu7n0Eg"},"source":["cat_data = pd.DataFrame(L, columns=[\"Animal\", \"Nr. of images\", \"Nr. of chars\", \"Summary\"])\n","cat_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_9m5ekSp6Lw"},"source":["cat_data.sort_values(\"Nr. of chars\", ascending=False).head(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7novabp5pghU"},"source":["cat_data.sort_values(\"Nr. of images\", ascending=False).head(4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxTUIMKIqEYj"},"source":["# Manx cat has longest article\n","# Persian cat has the most images "],"execution_count":null,"outputs":[]}]}