{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-APIs_Solutions.ipynb","provenance":[{"file_id":"1J5sCNUxmBh0t7MH3cVHColt9WFjca3ze","timestamp":1614455032952}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U3zPuM3zuwFT"},"source":["### Working with APIs##\n","#Exercises#"]},{"cell_type":"markdown","metadata":{"id":"ihjiiXFWCoOL"},"source":["\n","\n","---\n","\n","\n","<font color='violet'>\n","Hints are written in white, so you do not see them immediately. If you highlight them (or double-click on them), they will appear! \n","<font color='white'> I am a hint! :-)\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"U2hSLoyOBc1C"},"source":["## 1. Basic exercises"]},{"cell_type":"markdown","metadata":{"id":"bIhkFCLlKXQg"},"source":["Note: In these exercises, you will have to work with dictionaries, list comprehensions and loops quite often. If you don't remember/understand them very well, you should first go through the respective concepts once more!"]},{"cell_type":"markdown","metadata":{"id":"fTXm4BjVXSwH"},"source":["### Exercise 1.1"]},{"cell_type":"markdown","metadata":{"id":"TCiFg8zSZdXm"},"source":["Import the ``requests`` and the ``BeautifulSoup`` library."]},{"cell_type":"code","metadata":{"id":"-iACoS1aZj6F"},"source":["import requests\n","from bs4 import BeautifulSoup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kGpifrG5JoQT"},"source":["Use the Wikipedia API to retrieve the Wikipedia page on tigers (Tiger). "]},{"cell_type":"code","metadata":{"id":"R4lU1GPbKFML"},"source":["URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","PARAMS = {\n","    \"action\": \"parse\",\n","    \"page\": \"Tiger\",\n","    \"format\": \"json\",\n","}\n","\n","r = requests.get(url=URL, params=PARAMS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_TvWyHNKKGuJ"},"source":["Parse the JSON response object (so that it is converted to a Python dictionary)."]},{"cell_type":"code","metadata":{"id":"OYoqa3_CLPqN"},"source":["tiger = r.json()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uf2h-Q3SKGHv"},"source":["What is the URL you retrieved the data from? Type it into your browser and inspect the structure of the data."]},{"cell_type":"code","metadata":{"id":"sYqPRH2iLyla"},"source":["r.url"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVwXD41CSqZI"},"source":["Now inspect the keys of your dictionary (and those of the dictionary within the \"parse\" key)"]},{"cell_type":"code","metadata":{"id":"2G6hvLP1S1k3"},"source":["tiger.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVdGOVjkQJ0n"},"source":["tiger[\"parse\"].keys()  \n","# Note: To save some coding below, you could type: tiger = tiger[\"parse\"] "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgH3lOHBLSiC"},"source":["Now try to retrieve the following:\n","\n","1. The title of the page\n","2. All external links\n","3. All section headings (*extra task: all main section headings, i.e. headings on level 2)\n","4. The number of images in the article\n","5. All URLs to Wikipedia articles on tigers in other languages"]},{"cell_type":"code","metadata":{"id":"lgEBV3WFMIrr"},"source":["# 1. The title of the page\n","tiger[\"parse\"][\"title\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dg6HFN5aPpLD"},"source":["# 2. All external links\n","print(tiger[\"parse\"][\"externallinks\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VdjwNbMPuoB"},"source":["# 3. All section headings \n","sections = tiger[\"parse\"][\"sections\"]\n","[section[\"line\"] for section in sections]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Gw1ceGVR4Kf"},"source":["# 3. Extra task: all top level section headings (level 2)\n","[section[\"line\"] for section in sections if section[\"level\"]==\"2\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"inF7MgOuSkTD"},"source":["# 4. The number of images in the article\n","len(tiger[\"parse\"][\"images\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWbXZYakTcPf"},"source":["# 5. All URLs to Wikipedia articles on tigers in other languages\n","languages = tiger[\"parse\"][\"langlinks\"]\n","\n","langlinks = [elem[\"url\"] for elem in languages]\n","print(langlinks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JtVbFdNCA7gl"},"source":["Repetition (dictionaries and loops): Now make a dictionary of all languages for which a Wikipedia page exists as well as the titles of these pages. Use the languages as keys and the titles as values (`{\"English\":\"Tiger\", ...}`). How are tigers called in `Finnish`?"]},{"cell_type":"code","metadata":{"id":"zJMjH7_XBysE"},"source":["tiger_lang = {\"English\":\"Tiger\"}\n","for elem in languages:\n","  tiger_lang[elem[\"langname\"]] = elem[\"*\"]\n","tiger_lang"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQu3oZ-uC2Es"},"source":["tiger_lang[\"Finnish\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"buLd769eMKCh"},"source":["### Exercise 1.2"]},{"cell_type":"markdown","metadata":{"id":"bkHj5XAWMOCq"},"source":["Find the HTML text within the structured data on the tiger page and assign it to a variable named ``htmlText``. "]},{"cell_type":"code","metadata":{"id":"FUjqapl7MkmD"},"source":["htmlText = tiger[\"parse\"][\"text\"][\"*\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1XmSIG9YUAod"},"source":["Convert the string to a BeautifulSoup object and assign it to a variable called ``tigerSoup``."]},{"cell_type":"code","metadata":{"id":"zEa4QcVKUcX6"},"source":["tigerSoup = BeautifulSoup(htmlText)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gf87vEuiMmnV"},"source":["Repetitition: Extract (1) the first table and (2) the third paragraph of the article."]},{"cell_type":"code","metadata":{"id":"JiUQzXVnT0yn"},"source":["tigerSoup.find(\"table\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FD3QD_O_U4EF"},"source":["tigerSoup.find_all(\"p\")[2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7eCfljDUfCX"},"source":["### Exercise 1.3"]},{"cell_type":"markdown","metadata":{"id":"cxKlRIPqUkSP"},"source":["Retrieve the information about 10 Wikipedia pages that match with the word \"tiger\" and convert the response to a dictionary. ><font color='violet'> Hint: <font color='white'> Hint: The \"srlimit\" parameter allows you to specify how many pages you want to retrieve."]},{"cell_type":"code","metadata":{"id":"sSHEq35pUjgP"},"source":["# Perform page search \n","URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","PARAMS = {\n","    \"action\": \"query\",\n","    \"format\": \"json\",\n","    \"list\": \"search\",\n","    \"srsearch\": \"tiger\",\n","    \"srlimit\": 10\n","}\n","\n","r = requests.get(url=URL, params=PARAMS)\n","\n","# Convert to dictionary\n","tigers = r.json()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frOoUMARVXjs"},"source":["Navigate through the dictionary or type the URL into your browser to inspect how the data is structured."]},{"cell_type":"code","metadata":{"id":"-ZWDuClYV8pg"},"source":["# Get URL so you can explore the structure of the data in your Browser\n","r.url"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRB3JxMOVbcz"},"source":["# Explore the dictionary\n","print(tigers.keys())\n","print(tigers[\"query\"].keys())\n","type(tigers[\"query\"][\"search\"]) # tigers[\"query\"][\"search\"] contains a list of dictionaries!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqBrd_l1WzHA"},"source":["Now print out the titles of the 10 tiger pages."]},{"cell_type":"code","metadata":{"id":"S4WlSf6mWR9z"},"source":["# Extract titles\n","tiger_pages = tigers[\"query\"][\"search\"]\n","\n","[elem[\"title\"] for elem in tiger_pages]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdI5QiknVNDz"},"source":["### Exercise 1.4"]},{"cell_type":"markdown","metadata":{"id":"9QEITMxLVR1c"},"source":["In the web scraping exercises, you wrote a simple scraper that fetched you some information from the following animal pages: "]},{"cell_type":"code","metadata":{"id":"D8INxV1sVaY8"},"source":["animals = [\"Cat\", \"Dog\", \"Tiger\", \"Giant_panda\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gdiM6SHZVeQS"},"source":["You will now try to do the same using the API: Write a loop that fetches all the pages and retrieves (1) the title and (2) the number of images on each page.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lXWKGSPKeTM-"},"source":["\n","Let's do this step by step. First try to fetch the \"Cat\" page and convert the JSON response you get into a Python dictionary."]},{"cell_type":"code","metadata":{"id":"0GEF_PNHYOUH"},"source":["URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","PARAMS = {\n","    \"action\": \"parse\",\n","    \"page\": \"Cat\",\n","    \"format\": \"json\",\n","}\n","\n","r = requests.get(url=URL, params=PARAMS)\n","data = r.json()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERmyfnCOYQSy"},"source":["Now try to retrieve the title and the number of images. Assign each of them to a variable."]},{"cell_type":"code","metadata":{"id":"Wrnigsm2aiSV"},"source":["title = data[\"parse\"][\"title\"]\n","images = len(data[\"parse\"][\"images\"])\n","print(title, images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mO8qx6MRajDR"},"source":["Now write a loop that fetches all the pages and writes the response into a list. You will have to create an empty list and ``append`` the new elements to it:"]},{"cell_type":"code","metadata":{"id":"poZCR-AXarun"},"source":["L = []\n","for pagename in animals:\n","  URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","  PARAMS = {\n","        \"action\": \"parse\",\n","        \"page\": pagename,\n","        \"format\": \"json\",\n","        }\n","\n","  r = requests.get(url=URL, params=PARAMS)\n","  L.append(r)\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMPXXLfHbAgo"},"source":["Finally, you can bring everything together. Improve your loop so that it parses each page, retrieves the title and the number of images and writes them into a nested list."]},{"cell_type":"code","metadata":{"id":"vwkmGskAWXXD"},"source":["L = []\n","for pagename in animals:\n","  URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","  PARAMS = {\n","        \"action\": \"parse\",\n","        \"page\": pagename,\n","        \"format\": \"json\",\n","        }\n","\n","  r = requests.get(url=URL, params=PARAMS)\n","  data = r.json()\n","\n","  title = data[\"parse\"][\"title\"]\n","  images = len(data[\"parse\"][\"images\"])\n","\n","  L.append([title, images])\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I5fGuENsxWXa"},"source":["## 2. Advanced exercises*"]},{"cell_type":"markdown","metadata":{"id":"S4EPeADLLUwr"},"source":["---\n","\n","\n","<font color='red'>\n","*Feel free to skip the advanced exercises if you feel overwhelmed or if trying to solve the basic exercises already took you a lot of time! \n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YUZDdTvwVFzy"},"source":["### Exercise 2.1"]},{"cell_type":"markdown","metadata":{"id":"kSLGO3i4eapN"},"source":["Write a function called ``getWiki`` that allows you to enter the name of a Wikipedia page and returns the parsed JSON response as a Python dictionary."]},{"cell_type":"code","metadata":{"id":"iCYI6Z5cyqwN"},"source":["def getWiki(pagename):\n","  ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n","\n","  PARAMS = {\n","      \"action\": \"parse\",\n","      \"page\": pagename,\n","      \"format\": \"json\",\n","  }\n","\n","  r = requests.get(url=ENDPOINT, params=PARAMS)\n","  return r.json()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXkaa_dAfgdD"},"source":["getWiki(\"Python\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXI_caBbhoWq"},"source":["### Exercise 2.2"]},{"cell_type":"markdown","metadata":{"id":"7A6XtFz0iGgY"},"source":["You would like to know if Zürich or Bern is more popular on Wikipedia. For this purpose, you will measure (1) the number of Wikipedia articles within a 1km-radius around the train station and (2) the total number of images in these articles. Try to work with functions instead of copying and pasting code! \n","\n","You can take the following coordinates:\n","* Bern: 46.949722, 7.439444\n","* Zürich: 47.377455, 8.539688\n"]},{"cell_type":"markdown","metadata":{"id":"JQ3jkRkaTvkB"},"source":["Start by comparing the number of articles:"]},{"cell_type":"code","metadata":{"id":"q4VWjMzlat96"},"source":["# Define function to get names of all articles\n","\n","def getArts(coords):\n","\n","  URL = \"https://de.wikipedia.org/w/api.php\"  # Change to German Wikipedia!\n","  PARAMS = {\n","      \"format\": \"json\",\n","      \"list\": \"geosearch\",\n","      \"gscoord\": coords,\n","      \"gslimit\": \"max\",\n","      \"gsradius\": 1000, \n","      \"action\": \"query\"\n","  }\n","\n","  r = requests.get(url=URL, params=PARAMS)\n","  DATA = r.json()\n","  PLACES = DATA['query']['geosearch']\n","  results = [place[\"title\"] for place in PLACES]\n","  return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7lay61dMJCm"},"source":["bern = getArts(\"46.949722|7.439444\")\n","len(bern)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMhO1YbxMsGe"},"source":["zurich = getArts(\"47.377455|8.539688\")\n","len(zurich)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75HU6S8ZPezT"},"source":["Now try to compare the total number of images in these articles. You will have to write a loop to retrieve each page."]},{"cell_type":"code","metadata":{"id":"RJlt62p6PgQP"},"source":["# Function to get number of images\n","def nrImg(pagename):\n","  URL = \"https://de.wikipedia.org/w/api.php\"\n","\n","  PARAMS = {\n","      \"action\": \"parse\",\n","      \"page\": pagename,\n","      \"format\": \"json\",\n","  }\n","\n","  r = requests.get(url=URL, params=PARAMS)\n","  DATA = r.json()\n","  nr_images = len(DATA[\"parse\"][\"images\"])\n","  return nr_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SmbfGzIFQDw3"},"source":["# Compute total for Bern\n","bern_total = 0\n","\n","for i in bern:\n","  images = nrImg(i)\n","  bern_total += images\n","\n","print(bern_total) # total number of images\n","bern_total / len(bern) # average number of images per page"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_llCrZ__RMbC"},"source":["# Compute total for Zürich\n","zurich_total = 0\n","\n","for i in zurich:\n","  images = nrImg(i)\n","  zurich_total += images\n","\n","print(zurich_total) # total number of images\n","zurich_total / len(zurich) # average number of images per page"],"execution_count":null,"outputs":[]}]}